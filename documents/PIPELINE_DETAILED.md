# MÃ´ Táº£ Chi Tiáº¿t Pipeline Face Liveness Detection

## ğŸ“‹ Tá»•ng Quan Pipeline

Pipeline hiá»‡n táº¡i lÃ  má»™t **Static Ensemble SOTA** cho Face Liveness Detection, táº­p trung vÃ o viá»‡c phÃ¢n tÃ­ch áº£nh tÄ©nh vá»›i 2 nhÃ¡nh chÃ­nh: **Global Branch** (Frequency Analysis) vÃ  **Local Branch** (Patch-based Analysis).

Pipeline bao gá»“m 4 giai Ä‘oáº¡n chÃ­nh:
1. **Quality Gate** (TÃ¹y chá»n)
2. **Face Detection & Alignment** vá»›i Context Expansion
3. **Liveness Detection Ensemble** (Global + Local)
4. **Final Decision** vá»›i logic nghiÃªm ngáº·t

---

## ğŸ” Giai Äoáº¡n 1: Quality Gate (TÃ¹y Chá»n)

### Má»¥c ÄÃ­ch
Lá»c áº£nh Ä‘áº§u vÃ o Ä‘á»ƒ Ä‘áº£m báº£o cháº¥t lÆ°á»£ng trÆ°á»›c khi cháº¡y liveness detection.

### Quy TrÃ¬nh Chi Tiáº¿t

#### 1.1. Blur Detection (Laplacian Variance)

**CÆ¡ cháº¿:**
```python
# TÃ­nh toÃ¡n Ä‘á»™ sáº¯c nÃ©t cá»§a áº£nh
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
is_sharp = laplacian_var > blur_threshold  # Default: 100.0
```

**Giáº£i thÃ­ch:**
- **Laplacian operator** phÃ¡t hiá»‡n biáº¿n thiÃªn cÆ°á»ng Ä‘á»™ pixel
- áº¢nh sáº¯c nÃ©t cÃ³ **variance cao** (nhiá»u biáº¿n thiÃªn)
- áº¢nh má» cÃ³ **variance tháº¥p** (Ã­t biáº¿n thiÃªn)
- **Threshold**: 100.0 (cÃ³ thá»ƒ Ä‘iá»u chá»‰nh trong config)

**Káº¿t quáº£:**
- `blur_score < 100.0` â†’ Quality Gate **FAIL**
- `blur_score >= 100.0` â†’ Quality Gate **PASS**

#### 1.2. Pose Estimation (Yaw, Pitch, Roll)

**Kiá»ƒm tra gÃ³c quay máº·t:**
- **Yaw** < 20Â° (quay trÃ¡i/pháº£i)
- **Pitch** < 20Â° (ngáº©ng/cÃºi)
- **Roll** < 20Â° (nghiÃªng)

**Náº¿u vÆ°á»£t ngÆ°á»¡ng** â†’ Quality Gate **FAIL**

### Káº¿t Quáº£ Quality Gate

- **PASS**: Tiáº¿p tá»¥c pipeline
- **FAIL**: CÃ³ thá»ƒ bá» qua náº¿u dÃ¹ng `--skip-quality` flag

---

## ğŸ¯ Giai Äoáº¡n 2: Face Detection & Alignment

### 2.1. Face Detection (SCRFD)

#### Model: SCRFD (Sample and Computation Redistribution for Face Detection)

**ThÃ´ng tin:**
- Sá»­ dá»¥ng tá»« InsightFace `buffalo_l` pack
- Chá»‰ load **detection module** (`det_10g.onnx`), khÃ´ng load recognition
- **Input size**: 640x640
- **Output**: Bounding boxes + 5 keypoints

#### Quy TrÃ¬nh Detection:

```python
# SCRFD detect API
bboxes, kpss = model.detect(image, max_num=0, metric='default')
# bboxes: [N, 5] - [x1, y1, x2, y2, confidence]
# kpss: [N, 5, 2] - 5 landmarks [x, y] cho má»—i face
```

**5 Landmarks:**
1. **Left eye** (máº¯t trÃ¡i)
2. **Right eye** (máº¯t pháº£i)
3. **Nose tip** (Ä‘áº§u mÅ©i)
4. **Left mouth corner** (khÃ³e miá»‡ng trÃ¡i)
5. **Right mouth corner** (khÃ³e miá»‡ng pháº£i)

#### Confidence Filtering:
- Lá»c theo `conf_threshold` (default: 0.5)
- Chá»‰ giá»¯ faces cÃ³ `confidence >= threshold`

### 2.2. Context Expansion â­

#### Má»¥c ÄÃ­ch
Má»Ÿ rá»™ng bbox Ä‘á»ƒ bao gá»“m **context xung quanh** (viá»n giáº¥y, thiáº¿t bá»‹, ngÃ³n tay, background).

#### Quy TrÃ¬nh:

```python
# TÃ­nh center vÃ  size cá»§a bbox gá»‘c
center_x = (x1 + x2) / 2.0
center_y = (y1 + y2) / 2.0
width = x2 - x1
height = y2 - y1

# Má»Ÿ rá»™ng theo scale (default: 2.0)
new_width = width * context_expansion_scale  # 2.0x
new_height = height * context_expansion_scale  # 2.0x

# TÃ­nh bbox má»›i
x1_new = center_x - new_width / 2.0
y1_new = center_y - new_height / 2.0
x2_new = center_x + new_width / 2.0
y2_new = center_y + new_height / 2.0
```

**Scale**: 2.0 (bbox gá»‘c Ä‘Æ°á»£c má»Ÿ rá»™ng **2 láº§n**)

**Lá»£i Ã­ch:**
- âœ… PhÃ¡t hiá»‡n **viá»n giáº¥y in** (náº¿u káº» gian cáº§m áº£nh giÆ¡ lÃªn)
- âœ… PhÃ¡t hiá»‡n **viá»n Ä‘iá»‡n thoáº¡i/tablet** (náº¿u káº» gian giÆ¡ Ä‘iá»‡n thoáº¡i)
- âœ… Tháº¥y **ngÃ³n tay cáº§m thiáº¿t bá»‹**
- âœ… PhÃ¡t hiá»‡n **background bá»‹ biáº¿n dáº¡ng**

### 2.3. Face Alignment (Similarity Transform)

#### Má»¥c ÄÃ­ch
CÄƒn chá»‰nh khuÃ´n máº·t vá» **gÃ³c nhÃ¬n chuáº©n** Ä‘á»ƒ model dá»… phÃ¢n tÃ­ch.

#### Quy TrÃ¬nh:

```python
# Landmarks chuáº©n (theo InsightFace/ArcFace)
dst_landmarks = [
    [30.2946, 51.6963],  # left eye
    [65.5318, 51.5014],  # right eye
    [48.0252, 71.7366],  # nose
    [33.5493, 92.3655],  # left mouth
    [62.7299, 92.2041]   # right mouth
]

# TÃ­nh Affine Transform tá»« 3 Ä‘iá»ƒm (2 máº¯t + mÅ©i)
transform_matrix = cv2.getAffineTransform(src_points, dst_points)

# Ãp dá»¥ng transform
aligned_face = cv2.warpAffine(image, transform_matrix, (112, 112))
```

**Input**: Face ROI sau context expansion  
**Output**: 112x112 RGB, Ä‘Ã£ cÄƒn chá»‰nh  
**PhÆ°Æ¡ng phÃ¡p**: Affine Transform dÃ¹ng 3 Ä‘iá»ƒm (2 máº¯t + mÅ©i)

---

## ğŸ§  Giai Äoáº¡n 3: Liveness Detection Ensemble

### Tá»•ng Quan

Ensemble gá»“m **2 nhÃ¡nh chÃ­nh**:
- **Global Branch** (MiniFASNetV2): PhÃ¢n tÃ­ch toÃ n cá»¥c
- **Local Branch** (DeepPixBiS): PhÃ¢n tÃ­ch pixel-wise

**Temporal Branch** Ä‘Ã£ táº¯t (chá»‰ dÃ¹ng cho video/stream).

---

## ğŸ”¬ Model 1: Global Branch (MiniFASNetV2)

### Má»¥c TiÃªu

PhÃ¡t hiá»‡n dáº¥u hiá»‡u spoof á»Ÿ má»©c **toÃ n cá»¥c**:
- **MoirÃ© patterns** (lÆ°á»›i táº§n sá»‘ tá»« mÃ n hÃ¬nh)
- **Geometric distortions** (biáº¿n dáº¡ng do camera)
- **Phone bezels** (viá»n Ä‘iá»‡n thoáº¡i)
- **Screen reflections** (pháº£n xáº¡ mÃ n hÃ¬nh)

### Kiáº¿n TrÃºc Chi Tiáº¿t

#### Input Preprocessing:

```python
# 1. Convert BGR -> RGB
face_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)

# 2. Resize vá» 80x80 (input size cá»§a MiniFASNetV2)
face_resized = cv2.resize(face_rgb, (80, 80))

# 3. Normalize: (pixel / 255.0 - 0.5) / 0.5
# Káº¿t quáº£: pixel values trong range [-1, 1]
face_normalized = (face_resized.astype(np.float32) / 255.0 - 0.5) / 0.5

# 4. Convert to NCHW: [batch, channels, height, width]
face_input = np.expand_dims(face_normalized.transpose(2, 0, 1), axis=0)
# Shape: [1, 3, 80, 80]
```

#### Network Architecture:

```
Input: [1, 3, 80, 80]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv Block 1                        â”‚
â”‚ - Conv2d(3â†’16, kernel=3, stride=2)  â”‚ â†’ [1, 16, 40, 40]
â”‚ - BatchNorm2d(16)                   â”‚
â”‚ - ReLU                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv Block 2                        â”‚
â”‚ - Conv2d(16â†’32, kernel=3, stride=2) â”‚ â†’ [1, 32, 20, 20]
â”‚ - BatchNorm2d(32)                   â”‚
â”‚ - ReLU                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv Block 3                        â”‚
â”‚ - Conv2d(32â†’64, kernel=3, stride=2) â”‚ â†’ [1, 64, 10, 10]
â”‚ - BatchNorm2d(64)                   â”‚
â”‚ - ReLU                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv Block 4                        â”‚
â”‚ - Conv2d(64â†’128, kernel=3, stride=2)â”‚ â†’ [1, 128, 5, 5]
â”‚ - BatchNorm2d(128)                  â”‚
â”‚ - ReLU                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Global Average Pooling               â”‚
â”‚ - AdaptiveAvgPool2d(1)               â”‚ â†’ [1, 128, 1, 1]
â”‚ - Flatten                            â”‚ â†’ [1, 128]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Classifier                           â”‚
â”‚ - Linear(128â†’64)                     â”‚ â†’ [1, 64]
â”‚ - ReLU                               â”‚
â”‚ - Dropout(0.5)                       â”‚
â”‚ - Linear(64â†’2)                      â”‚ â†’ [1, 2]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Output: [1, 2] - [fake_score, real_score]
```

#### Chi Tiáº¿t Tá»«ng Layer:

1. **Conv Block 1** (3â†’16 channels):
   - Kernel: 3x3, stride=2, padding=1
   - Giáº£m kÃ­ch thÆ°á»›c: 80x80 â†’ 40x40
   - TÄƒng channels: 3 â†’ 16

2. **Conv Block 2** (16â†’32 channels):
   - 40x40 â†’ 20x20
   - Channels: 16 â†’ 32

3. **Conv Block 3** (32â†’64 channels):
   - 20x20 â†’ 10x10
   - Channels: 32 â†’ 64

4. **Conv Block 4** (64â†’128 channels):
   - 10x10 â†’ 5x5
   - Channels: 64 â†’ 128

5. **Global Average Pooling**:
   - 5x5 â†’ 1x1
   - Táº¡o feature vector 128D

6. **Classifier**:
   - Linear(128â†’64) + ReLU + Dropout(0.5)
   - Linear(64â†’2) â†’ [fake_score, real_score]

#### Output Processing:

```python
# Output tá»« model: [batch, 2]
if len(outputs[0].shape) == 2:
    score = float(outputs[0][0][1])  # Láº¥y real_score (index 1)
else:
    # Fallback: náº¿u output format khÃ¡c
    score = float(outputs[0].flatten()[0])
    if score < 0.5:
        score = 1.0 - score  # Äáº£o ngÆ°á»£c náº¿u lÃ  fake_score

# Clamp vá» [0, 1]
score = max(0.0, min(1.0, score))
```

#### Táº¡i Sao Hiá»‡u Quáº£?

- âœ… **NhÃ¬n toÃ n cá»¥c**: PhÃ¡t hiá»‡n patterns trÃªn toÃ n áº£nh (MoirÃ©, viá»n thiáº¿t bá»‹)
- âœ… **Frequency analysis**: Conv layers há»c cÃ¡c táº§n sá»‘ Ä‘áº·c trÆ°ng
- âœ… **Lightweight**: 80x80 input, Ã­t tham sá»‘, inference nhanh

---

## ğŸ” Model 2: Local Branch (DeepPixBiS)

### Má»¥c TiÃªu

PhÃ¢n tÃ­ch **pixel-wise** Ä‘á»ƒ phÃ¡t hiá»‡n:
- **Skin texture** (káº¿t cáº¥u da tháº­t vs in/mÃ n hÃ¬nh)
- **Screen pixels** (pixel grid cá»§a mÃ n hÃ¬nh)
- **Reflections** (pháº£n xáº¡ trÃªn bá» máº·t)
- **Print artifacts** (artifacts tá»« in áº¥n)

### Kiáº¿n TrÃºc Chi Tiáº¿t

#### Input Preprocessing:

```python
# 1. Convert BGR -> RGB
face_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)

# 2. Resize vá» 224x224 (input size cá»§a DeepPixBiS)
face_resized = cv2.resize(face_rgb, (224, 224))

# 3. Normalize: (pixel / 255.0 - 0.5) / 0.5
face_normalized = (face_resized.astype(np.float32) / 255.0 - 0.5) / 0.5

# 4. Convert to NCHW
face_input = np.expand_dims(face_normalized.transpose(2, 0, 1), axis=0)
# Shape: [1, 3, 224, 224]
```

#### Network Architecture:

```
Input: [1, 3, 224, 224]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv Block 1                        â”‚
â”‚ - Conv2d(3â†’64, kernel=7, stride=2)   â”‚ â†’ [1, 64, 112, 112]
â”‚ - BatchNorm2d(64)                    â”‚
â”‚ - ReLU                               â”‚
â”‚ - MaxPool2d(kernel=3, stride=2)      â”‚ â†’ [1, 64, 56, 56]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv Block 2 (2 layers)              â”‚
â”‚ - Conv2d(64â†’128, stride=2)           â”‚ â†’ [1, 128, 28, 28]
â”‚ - BatchNorm2d(128)                   â”‚
â”‚ - ReLU                               â”‚
â”‚ - Conv2d(128â†’128, stride=1)          â”‚
â”‚ - BatchNorm2d(128)                   â”‚
â”‚ - ReLU                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv Block 3 (2 layers)              â”‚
â”‚ - Conv2d(128â†’256, stride=2)          â”‚ â†’ [1, 256, 14, 14]
â”‚ - BatchNorm2d(256)                  â”‚
â”‚ - ReLU                               â”‚
â”‚ - Conv2d(256â†’256, stride=1)          â”‚
â”‚ - BatchNorm2d(256)                   â”‚
â”‚ - ReLU                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv Block 4 (2 layers)              â”‚
â”‚ - Conv2d(256â†’512, stride=2)          â”‚ â†’ [1, 512, 7, 7]
â”‚ - BatchNorm2d(512)                  â”‚
â”‚ - ReLU                               â”‚
â”‚ - Conv2d(512â†’512, stride=1)          â”‚
â”‚ - BatchNorm2d(512)                   â”‚
â”‚ - ReLU                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pixel    â”‚      â”‚ Binary   â”‚
â”‚ Head     â”‚      â”‚ Head     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“                   â†“
[1,1,14,14]      [1, 2]
```

#### Pixel Head (Pixel-wise Map):

```
Input: [1, 512, 7, 7]

Conv2d(512â†’256, kernel=3, padding=1) â†’ [1, 256, 7, 7]
BatchNorm2d(256)
ReLU

Conv2d(256â†’128, kernel=3, padding=1) â†’ [1, 128, 7, 7]
BatchNorm2d(128)
ReLU

Conv2d(128â†’1, kernel=1) â†’ [1, 1, 7, 7]
Sigmoid() â†’ [1, 1, 7, 7]

Upsample to 14x14 â†’ [1, 1, 14, 14]
```

**Output**: Pixel map 14x14, má»—i pixel lÃ  xÃ¡c suáº¥t spoof táº¡i vÃ¹ng Ä‘Ã³  
**GiÃ¡ trá»‹ cao** = spoof region, **tháº¥p** = real region

#### Binary Head (Global Classification):

```
Input: [1, 512, 7, 7]

AdaptiveAvgPool2d(1) â†’ [1, 512, 1, 1]
Flatten â†’ [1, 512]

Linear(512â†’256) â†’ [1, 256]
ReLU
Dropout(0.5)

Linear(256â†’2) â†’ [1, 2]
```

**Output**: [fake_score, real_score] cho toÃ n áº£nh

#### Output Processing:

```python
# DeepPixBiS output: [batch, 1, H, W] pixel map
if len(outputs[0].shape) == 4:
    pixel_map = outputs[0][0, 0]  # Shape: (H, W) - thÆ°á»ng 14x14
else:
    pixel_map = outputs[0][0].reshape(14, 14)

# TÃ­nh average score tá»« pixel map
raw_score = float(np.mean(pixel_map))

# Normalize vá» [0, 1]
# Giáº£ sá»­ raw_score cao = real (tÃ¹y model training)
score = raw_score
score = max(0.0, min(1.0, score))  # Clamp

return score, pixel_map
```

#### Táº¡i Sao Hiá»‡u Quáº£?

- âœ… **Pixel-wise supervision**: Há»c tá»«ng vÃ¹ng, phÃ¡t hiá»‡n chi tiáº¿t
- âœ… **Dual output**: Pixel map + binary classification
- âœ… **Texture analysis**: PhÃ¡t hiá»‡n káº¿t cáº¥u da tháº­t vs in/mÃ n hÃ¬nh
- âœ… **High resolution**: 224x224 input, chi tiáº¿t hÆ¡n Global Branch

---

## ğŸ¯ Giai Äoáº¡n 4: Ensemble Fusion & Final Decision

### 4.1. Weighted Sum Fusion

#### CÃ´ng Thá»©c:

```python
final_score = (
    weight_global * global_score +    # 0.5 * global_score
    weight_local * local_score +       # 0.5 * local_score
    weight_temporal * temporal_score   # 0.0 (disabled)
)
```

**Weights**: Global = 0.5, Local = 0.5 (temporal disabled)  
**Final score**: [0, 1], 1 = real, 0 = spoof

### 4.2. Logic NghiÃªm Ngáº·t âš ï¸

#### YÃªu Cáº§u:

```python
# 1. Cáº£ 2 branch pháº£i pass threshold riÃªng
global_passed = global_score > global_threshold  # 0.5
local_passed = local_score > local_threshold     # 0.5

# 2. Final score pháº£i > final_threshold
final_score_passed = final_score > final_threshold  # 0.12

# 3. Cáº¢ 2 Ä‘iá»u kiá»‡n pháº£i Ä‘Ãºng
is_real = (global_passed AND local_passed) AND final_score_passed
```

**LÃ½ do**: TrÃ¡nh false positive khi 1 branch bá»‹ lá»—i  
**Káº¿t quáº£**: YÃªu cáº§u cáº£ 2 branch Ä‘á»“ng thuáº­n

### 4.3. Thresholds

- **Global threshold**: 0.5
- **Local threshold**: 0.5
- **Final threshold**: 0.12 (tá»‘i Æ°u trÃªn test set)

---

## ğŸ“Š So SÃ¡nh 2 Models

| TiÃªu ChÃ­ | Global Branch (MiniFASNetV2) | Local Branch (DeepPixBiS) |
|----------|------------------------------|---------------------------|
| **Input size** | 80x80 | 224x224 |
| **Má»¥c tiÃªu** | Frequency patterns, geometric distortions | Skin texture, pixel artifacts |
| **Output** | Binary classification [fake, real] | Pixel map (14x14) + binary |
| **Strengths** | PhÃ¡t hiá»‡n MoirÃ©, viá»n thiáº¿t bá»‹ | PhÃ¡t hiá»‡n texture, reflections |
| **Weaknesses** | KÃ©m chi tiáº¿t, dá»… miss texture | Cháº­m hÆ¡n, cáº§n input lá»›n |
| **Weight** | 0.5 | 0.5 |

---

## ğŸ”„ Tá»•ng Káº¿t Pipeline Flow

```
Input Image (BGR)
    â†“
[Quality Gate] (Optional)
    â”œâ”€ Blur Detection (Laplacian)
    â””â”€ Pose Estimation (Yaw/Pitch/Roll)
    â†“
[Face Detection] (SCRFD)
    â”œâ”€ Detect faces + 5 landmarks
    â””â”€ Confidence filtering
    â†“
[Context Expansion] (Scale 2.0)
    â”œâ”€ Expand bbox 2x
    â””â”€ Include context (viá»n, thiáº¿t bá»‹, ngÃ³n tay)
    â†“
[Face Alignment] (Similarity Transform)
    â”œâ”€ Align using 3 points (2 máº¯t + mÅ©i)
    â””â”€ Output: 112x112 aligned face
    â†“
[Ensemble Prediction]
    â”œâ”€ Global Branch (MiniFASNetV2)
    â”‚   â”œâ”€ Input: 80x80
    â”‚   â”œâ”€ Output: global_score [0, 1]
    â”‚   â””â”€ PhÃ¡t hiá»‡n: MoirÃ©, viá»n, distortions
    â”‚
    â””â”€ Local Branch (DeepPixBiS)
        â”œâ”€ Input: 224x224
        â”œâ”€ Output: local_score [0, 1] + pixel_map [14x14]
        â””â”€ PhÃ¡t hiá»‡n: Texture, reflections, artifacts
    â†“
[Fusion]
    â”œâ”€ Weighted Sum: 0.5 * global + 0.5 * local
    â””â”€ Final Score [0, 1]
    â†“
[Final Decision]
    â”œâ”€ Check: global_passed AND local_passed
    â”œâ”€ Check: final_score > threshold
    â””â”€ Output: REAL or SPOOF
```

---

## ğŸ¯ Káº¿t Luáº­n

Pipeline nÃ y káº¿t há»£p:
- **Global Branch**: PhÃ¡t hiá»‡n patterns toÃ n cá»¥c (MoirÃ©, viá»n)
- **Local Branch**: PhÃ¢n tÃ­ch chi tiáº¿t pixel-wise (texture, artifacts)
- **Context Expansion**: Bao gá»“m context xung quanh
- **Logic nghiÃªm ngáº·t**: YÃªu cáº§u cáº£ 2 branch Ä‘á»“ng thuáº­n

**Káº¿t quáº£**: Äá»™ chÃ­nh xÃ¡c cao, giáº£m false positive, phÃ¹ há»£p vá»›i áº£nh tÄ©nh.

---

## ğŸ“ Config Parameters

### Detection
- `context_expansion_scale: 2.0` - Má»Ÿ rá»™ng bbox 2x Ä‘á»ƒ tháº¥y context

### Liveness Ensemble
- `global_branch.threshold: 0.5` - Threshold cho Global Branch
- `local_branch.threshold: 0.5` - Threshold cho Local Branch
- `global_branch.weight: 0.5` - Weight trong fusion
- `local_branch.weight: 0.5` - Weight trong fusion
- `final_threshold: 0.12` - Threshold cuá»‘i cÃ¹ng (tá»‘i Æ°u trÃªn test set)
- `fusion_method: weighted_sum` - PhÆ°Æ¡ng phÃ¡p fusion

### Quality Gate
- `blur_threshold: 100.0` - NgÆ°á»¡ng blur detection
- `max_yaw: 20.0` - GÃ³c quay ngang tá»‘i Ä‘a
- `max_pitch: 20.0` - GÃ³c quay dá»c tá»‘i Ä‘a
- `max_roll: 20.0` - GÃ³c nghiÃªng tá»‘i Ä‘a

---

## ğŸš€ Usage

```bash
# Inference vá»›i ensemble
python src/inference_ensemble.py \
    --image data/test/normal/22_1.jpg \
    --config config/config.yaml \
    --skip-quality

# Output:
# - Global Branch score
# - Local Branch score
# - Final Score
# - Prediction: REAL or SPOOF
```

---

**TÃ i liá»‡u nÃ y mÃ´ táº£ chi tiáº¿t toÃ n bá»™ pipeline Face Liveness Detection, tá»« input Ä‘áº¿n output cuá»‘i cÃ¹ng.**

